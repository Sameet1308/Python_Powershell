import re
import sqlparse
from sqlparse.sql import IdentifierList, Identifier
from sqlparse.tokens import Keyword, DML

# Function to read the log file and remove timestamps and line numbers
def read_and_clean_log(file_path):
    cleaned_content = []
    try:
        with open(file_path, 'r') as file:
            for line in file:
                # Remove the timestamp and line numbers at the beginning of each line
                # Assume timestamps are like '040T50323.109-0400' and lines may start with numbers like '0030'
                cleaned_line = re.sub(r'^\S+\s+\d+\s+', '', line).strip()
                cleaned_content.append(cleaned_line)
    except FileNotFoundError:
        print(f"The file at {file_path} was not found.")
    except Exception as e:
        print(f"An error occurred: {e}")
    
    return cleaned_content

# Function to extract SQL details from cleaned log content
def extract_sql_details(content):
    sql_statements = []
    current_query = []
    capturing = False
    
    for line in content:
        if 'SQL' in line:  # Assuming 'SQL' indicates the start of an SQL statement
            capturing = True
            current_query = [line]
        elif capturing and ';' in line:  # Assuming ';' is the end of an SQL statement
            current_query.append(line)
            sql_statements.append(' '.join(current_query))
            capturing = False
            current_query = []
        elif capturing:
            current_query.append(line)

    # Parsing each SQL statement to extract details
    for statement in sql_statements:
        parsed = sqlparse.parse(statement)[0]
        tables = []
        columns = []

        for token in parsed.tokens:
            if token.ttype is DML and token.value.upper() == 'SELECT':
                continue
            if isinstance(token, IdentifierList):
                for identifier in token.get_identifiers():
                    if identifier.get_parent_name() == 'FROM':
                        tables.append(identifier.get_real_name())
                    else:
                        columns.append(identifier.get_real_name())
            elif isinstance(token, Identifier):
                if token.get_parent_name() == 'FROM':
                    tables.append(token.get_real_name())
                else:
                    columns.append(token.get_real_name())
            elif token.ttype is Keyword and token.value.upper() == 'FROM':
                index = parsed.token_index(token) + 1
                if isinstance(parsed.tokens[index], Identifier):
                    tables.append(parsed.tokens[index].get_real_name())

        print(f"Tables: {tables}")
        print(f"Columns: {columns}")

# Main script execution
log_file_path = '/path/to/your/log_file.log'  # Replace with the actual path to your log file
cleaned_content = read_and_clean_log(log_file_path)
extract_sql_details(cleaned_content)
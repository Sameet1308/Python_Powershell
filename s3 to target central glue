import boto3
import pandas as pd
from io import StringIO

# Initialize boto3 clients
s3_client = boto3.client('s3')
glue_client = boto3.client('glue')

# Define the S3 bucket and prefix
SOURCE_BUCKET = 'your-source-bucket'
SOURCE_PREFIX = 'your/source/prefix/'

# Define the target Glue catalog and database
TARGET_ACCOUNT_ID = 'target-account-id'
TARGET_DB_NAME = 'target-db-name'

def get_csv_files(bucket, prefix):
    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)
    return [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')]

def process_csv_data(data):
    df = pd.read_csv(StringIO(data))
    df = df.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)
    return df

def infer_schema(df):
    schema = []
    for column, dtype in df.dtypes.items():
        if dtype == 'object':
            col_type = 'string'
        elif dtype == 'int64':
            col_type = 'int'
        elif dtype == 'float64':
            col_type = 'double'
        else:
            col_type = 'string'
        schema.append({'Name': column, 'Type': col_type})
    return schema

def create_glue_table(glue_client, database_name, table_name, columns, location):
    glue_client.create_table(
        DatabaseName=database_name,
        TableInput={
            'Name': table_name,
            'StorageDescriptor': {
                'Columns': columns,
                'Location': location,
                'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',
                'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',
                'SerdeInfo': {
                    'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',
                    'Parameters': {
                        'field.delim': ',',
                        'escape.delim': '\\',
                        'quote.delim': '"'
                    }
                }
            },
            'TableType': 'EXTERNAL_TABLE'
        }
    )

def lambda_handler(event, context):
    csv_files = get_csv_files(SOURCE_BUCKET, SOURCE_PREFIX)
    
    for file_key in csv_files:
        # Read CSV file from S3
        response = s3_client.get_object(Bucket=SOURCE_BUCKET, Key=file_key)
        data = response['Body'].read().decode('utf-8')
        
        # Process CSV data to remove double quotes
        df = process_csv_data(data)
        
        # Infer schema from the DataFrame
        columns = infer_schema(df)
        
        # Table name based on the file name
        table_name = file_key.split('/')[-1].replace('.csv', '_clean')
        
        # Create Glue table in the target database
        create_glue_table(glue_client, TARGET_DB_NAME, table_name, columns, f's3://{SOURCE_BUCKET}/{file_key}')
    
    return {
        'statusCode': 200,
        'body': f'Successfully processed {len(csv_files)} files.'
    }
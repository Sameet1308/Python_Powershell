import pandas as pd
from sqlalchemy import create_engine
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import warnings

# Suppress the DtypeWarning
warnings.simplefilter(action='ignore', category=pd.errors.DtypeWarning)

# Connection parameters
server = 'b18382.hxhx.adid.net'
port = '2828'
database = 'YOUR_DATABASE'
table_name = 'YOUR_TABLE_NAME'  # Just the table name without schema
csv_file_path = r'\\ghh\path\to\your\large_csv_file.csv'  # Adjust this to your network share path
chunk_size = 10000  # Reasonable chunk size to avoid overloading the system
sample_size = 100000  # Number of rows to load for sample measurement

# Create a connection to the SQL Server database using Windows Authentication
connection_string = f"mssql+pyodbc://@{server}:{port}/{database}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes"
engine = create_engine(connection_string)

# Function to load a single chunk of data
def load_chunk(chunk, table_name):
    chunk.to_sql(table_name, engine, if_exists='append', index=False, schema='dbo')

# Measure time for sample data
start_time = time.time()

# Load sample data
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = []
    for chunk in pd.read_csv(csv_file_path, chunksize=chunk_size, nrows=sample_size):
        futures.append(executor.submit(load_chunk, chunk, table_name))
    for future in as_completed(futures):
        future.result()

end_time = time.time()
sample_duration = end_time - start_time

# Calculate estimated time for full data
total_rows = 2000000  # Total number of rows in the CSV
estimated_total_time = (total_rows / sample_size) * sample_duration

print(f"Sample load time for {sample_size} rows: {sample_duration:.2f} seconds")
print(f"Estimated total time for {total_rows} rows: {estimated_total_time:.2f} seconds")

# Define a function to load CSV in parallel chunks and write to SQL Server
def load_csv_to_sql_in_parallel(csv_file, table_name, chunk_size):
    start_time = time.time()  # Start the timer

    # Create table if it doesn't exist
    df = pd.read_csv(csv_file, nrows=1)
    df.head(0).to_sql(table_name, engine, if_exists='replace', index=False, schema='dbo')

    # Use ThreadPoolExecutor for parallel processing
    with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust number of workers as needed
        futures = []
        for chunk in pd.read_csv(csv_file, chunksize=chunk_size):
            futures.append(executor.submit(load_chunk, chunk, table_name))
        
        # Wait for all futures to complete
        for future in as_completed(futures):
            future.result()  # This will raise exceptions if any occurred

    end_time = time.time()  # End the timer
    duration = end_time - start_time
    print(f"Total time taken: {duration:.2f} seconds")

# Load CSV to SQL Server
load_csv_to_sql_in_parallel(csv_file_path, table_name, chunk_size)

print("CSV file has been successfully loaded into the SQL Server table.")